# ML System Design Document: Harry Potter RAG

## 1. Зачем идем в разработку продукта?

Цель проекта — создать интерактивную систему, позволяющую пользователям задавать вопросы по сюжету, персонажам и деталям книг серии «Гарри Поттер» и получать точные ответы с прямыми цитатами из текста и указанием источника (название книги и номер главы). Это решает проблему трудоёмкого поиска информации в объёмных текстах и обеспечивает прозрачность: пользователь видит, откуда взят ответ. Система включает backend (FastAPI) и frontend (React), а генерация ответов выполняется с помощью Mistral AI API (бесплатная версия)

## 2. Бизнес-требования и ограничения

*   **Функциональные требования:**
    *   Система принимает текстовый вопрос от пользователя через веб-интерфейс (React).
    *   Backend (FastAPI) обрабатывает запрос, ищет релевантные фрагменты в книгах с сохранением метаданных (`book`, `chapter`).
    *   Система формирует промпт и отправляет его в **Mistral AI API**.
    *   Полученный ответ структурируется и возвращается с **прямой цитатой** и **ссылкой на источник** (например: *«Гарри Поттер и Орден Феникса, глава 37»*).
    *   Ответ отображается на фронтенде.

*   **Нефункциональные требования:**
    *   Ответы должны быть основаны **только на предоставленном контексте** (ретривнутых фрагментах).
    *   Время отклика — до **10–15 секунд** (с учётом задержек Mistral API).
    *   MVP должен быть реализован за **2 месяца**.
    *   Качество будет оценено вручную на **30–50 вопросах**; целевой порог — ≥80% ответов с корректной цитатой и источником.

*   **Ограничения:**
    *   Используются только тексты книг (7 томов), полученные из открытых источников (например, [Flibusta](https://flibusta.site)) **в образовательных целях**.
    *   Зависимость от **Mistral AI API**: требуется интернет и API-ключ.
    *   Бесплатный тариф Mistral может иметь **ограничения по количеству запросов/токенов в минуту** — система должна обрабатывать ошибки (429 Too Many Requests).
    *   Поддержка только одного языка (язык исходного текста: русский или английский).
    *   На этапе MVP не предполагается поддержка истории диалога или сложной логики.

## 3. Что входит в скоуп проекта/итерации, что не входит

*   **Входит:**
    *   Предобработка текстов: парсинг, разбиение по главам, извлечение метаданных (`book`, `chapter`).
    *   Векторизация чанков с привязкой к метаданным.
    *   Локальное векторное хранилище (FAISS или Chroma).
    *   RAG-пайплайн: ретривер + формирование промпта + вызов **Mistral API**.
    *   Backend на **FastAPI** с эндпоинтом `/ask`, включающим обработку ошибок API.
    *   Frontend на **React** с полем ввода и отображением: ответ, цитата, источник.
    *   Использование **бесплатной версии Mistral API** (модель, например, `mistral-small-latest` или `open-mistral-7b`).

*   **Не входит:**
    *   Локальный запуск LLM (всё генерация — через Mistral API).
    *   Кэширование ответов или запросов (в MVP).
    *   Мультиязычная поддержка.
    *   История чата, авторизация, админка.
    *   Поддержка других серий книг или внешних источников (вики, фильмы).

## 4. Предпосылки решения

Mistral AI предоставляет бесплатный доступ к мощным open-weight моделям с низкой задержкой и хорошим качеством генерации. Это позволяет сосредоточиться на RAG-логике, а не на развертывании LLM локально. Наличие структурированного текста с главами даёт возможность точного цитирования. Архитектура FastAPI + React обеспечивает чёткое разделение backend и frontend.

## 5. Постановка задачи

Реализовать систему, которая по вопросу пользователя (например, *«Почему Добби освободился от семьи Малфоев?»*) возвращает структурированный ответ:

> **Ответ**: Добби освободился, потому что Люциус Малфой случайно дал ему носок — предмет одежды, что по законам домовиков означает освобождение.  
> **Цитата**: *«Ты не должен был давать мне носок!» — воскликнул Люциус Малфой, глядя на Добби с ненавистью.*  
> **Источник**: *«Гарри Поттер и Тайная комната», глава 18*

**Основная метрика успеха**:  
— ≥80% ответов на тестовом наборе из 30–50 вопросов содержат релевантную цитату и корректно указанную книгу и главу, на вопросы, не указанные в книге нужен ответ формата "Не знаю".

### 5.1. Ручная оценка
- **Объём**: 30–50 вопросов, охватывающих факты, сюжет, мотивации персонажей.
- **Трудозатраты**: ~2 минуты на вопрос → **1–1.5 часа** на одного оценщика.
- **Процедура**: два участника независимо оценивают; согласованность фиксируется.

### 5.2. Автоматизированная оценка (опционально)
- `hit_rate@5` для ретривера.
- Логирование ошибок Mistral API (429, 5xx) для анализа надёжности.

> Финальное решение о качестве — по результатам ручной оценки.

### 3. Этапы решения задачи

#### Этап 1. Подготовка данных

*   **Описание данных/сущностей:**
    *   **Источник:** Тексты семи книг серии «Гарри Поттер» на русском языке в формате `.txt`.
    *   **Сущности:** Главы книг. Каждая глава разбивается на чанки для поиска.
    *   **Проблемы и риски:**
        1.  **Качество текста:** Артефакты форматирования (`[\u2028\u2029\xA0]`) могут мешать парсингу. Обрабатывается в `data_preparation.py`.
        2.  **Объем данных:** 197 глав (~1.2 млн слов) - достаточно для задачи.
        3.  **Разметка:** Отсутствует. Система полагается на точность контекста.
    *   **Процесс генерации данных:**
        *   **Поступление:** `.txt` файлы в папке `/data`.
        *   **Формат:** Строго "Глава X".
        *   **Процесс:** При старте системы вызывается `load_all_books()` → `parse_book_to_chapters()` → создает объекты `Document` с метаданными `{book, chapter}`.
    *   **Необходимость дополнительных данных:** Не требуется.
    *   **Конфиденциальная информация:** Нет.
*   **Необходимый результат этапа:** Список документов по главам для семантического чанкинга.

#### **Этап 2. Подготовка прогнозных моделей**

*   **ML-метрики:** **Accuracy** — доля правильно отвеченных вопросов. Обоснование: цель — фактическая точность.
*   **Схема валидации:** Hold-out на `valid_questions.json`. **Ручная проверка** (`is_correct`) обязательна из-за семантической эквивалентности.
*   **Бейзлайн:**
    1.  **Предобработка запроса:** Расширение ключевыми словами через LLM.
    2.  **Поиск:** Гибридный (Chroma + BM25) с RRF.
    3.  **Чанкинг:** `SemanticChunker` с порогом 600 слов.
    4.  **Генерация:** Mistral AI с жестким промптом ("Только по контексту").
*   **Стратегии развития:**
    1.  Настройка `similarity_threshold`, весов RRF.
    2.  Улучшение промпта (few-shot).
    3.  Авто-подбор гиперпараметров.
*   **Анализ модели:** По `results.json`. Ошибки типа "Не знаю" анализируются на предмет проблем с поиском или генерацией.
*   **Риски:**
    *   **Hallucinations:** Снижение — строгий промпт и ручная проверка.
    *   **Неточный поиск:** Снижение — гибридный поиск.
    *   **Зависимость от API:** Снижение — повторные попытки.
*   **Необходимый результат:** Файл `results.json` с точностью ≥80%.

#### **Этапы, специфические для задачи RAG**

*   **Этап 3. Управление контекстом и генерация ответов**
    *   **Описание:** Критический этап для качества.
    *   **Техника:**
        1.  **Извлечение контекста:** Топ-15 чанков объединяются.
        2.  **Форматирование:** Явное указание `[ИСТОЧНИК: ...]`.
        3.  **Генерация:** Температура 0.0 для детерминированности.
    *   **Риски:**
        *   Длинный ответ - снижение: `temperature=0.0`.
        *   Нет ответа в контексте - снижение: инструкция "Не знаю".
    *   **Необходимый результат:** Ответ либо с цитатой, либо "Не знаю".

## 6. Блок-схема решения

[Пользователь вводит вопрос в React-интерфейсе]
                     ↓
       [POST /ask → FastAPI backend]
                     ↓
[Retriever: поиск топ-K чанков + извлечение metadata (book, chapter)]
                     ↑
[Векторное хранилище ← Тексты книг, размеченные по главам]
                     ↓
[Формирование промпта для Mistral API:
 "Ответь ТОЛЬКО на основе приведённых ниже отрывков.
  Обязательно включи ПРЯМУЮ ЦИТАТУ из текста и укажи НАЗВАНИЕ КНИГИ и НОМЕР ГЛАВЫ.
  Не выдумывай информацию."]
                     ↓
        [Запрос к Mistral AI API (через HTTP)]
                     ↓
    [Обработка ответа: извлечение/валидация цитаты и источника]
                     ↓
 [FastAPI возвращает JSON: {answer: str, quote: str, source: str}]
                     ↓
   [React отображает ответ, цитату и источник пользователю]

### 3. Подготовка пилота

#### 3.1. Способ оценки пилота
Пилот проводится в форме ручной экспертной оценки на фиксированном наборе из 30–50 заранее подготовленных вопросов (valid_questions.json). Каждый вопрос проверяется на:

Наличие фактически верного ответа,
Присутствие точной цитаты из текста,
Корректность указания названия книги и номера главы.
Оценка выполняется независимо двумя экспертами. При расхождении — привлекается третий.
Используется бинарная метрика: is_correct = true/false.

#### 3.2. Что считаем успешным пилотом
Пилот считается успешным, если:

≥80% ответов признаны корректными (accuracy ≥ 0.8),
Ни один ответ не содержит галлюцинаций (выдуманной информации),
На вопросы, не имеющие ответа в тексте, возвращается фраза «Не знаю».
Если пилот успешен — система готова к демонстрации.

#### 3.3. Подготовка пилота

Вычислительные затраты: модель не обучается на GPU — используется только Mistral API и локальный ретривер (Chroma + BM25). Стоимость пилота — только тариф Mistral API, который для MVP бесплатен.

Перед запуском пилота:
Запускается load_all_books() → индексация всех глав (~2–3 мин на CPU),
Система поднимается через FastAPI,
Выполняется прогон всех вопросов через скрипт evaluate.py,
Генерируется results.json,
Проводится ручная оценка.

Ограничений по вычислительной сложности нет: ретривер работает локально, а генерация — в облаке. 
Могут потребоваться затраты на публикацию решения на сервере.

## 4. Внедрение для production систем

### 4.1. Архитектура решения

#### Диаграмма архитектуры технического решения


+------------------+       +---------------------+       +------------------+
|   Пользователь   | ----> |   React Frontend    | ----> |   FastAPI Backend|
| (браузер)        | HTTP  | (localhost:5173 или | HTTP  | (localhost:8000) |
+------------------+       |  dockerized:8080)   |       +------------------+
                           +----------+----------+                 |
                                      |                            |
                                      |                            v
                           +----------v----------+      +----------------------+
                           | Mistral AI Cloud API| <--- | RAG Pipeline         |
                           | (mistral-small-latest)|    | - Гибридный поиск    |
                           +---------------------+      |   (Chroma + BM25)    |
                                                        | - Семантический чанкинг|
                                                        | - Расширение запроса |
                                                        +----------------------+
                                                                 |
                                                                 v
                                                 +-------------------------------+
                                                 | Локальное векторное хранилище |
                                                 | Chroma + метаданные книг     |
                                                 +-------------------------------+

#### Компоненты и их назначение:

- **React Frontend**: отображает интерфейс дневника Тома Реддла, отправляет вопросы на backend, получает и отображает ответы.
- **FastAPI Backend**: обрабатывает входящие POST-запросы (`/ask`), управляет жизненным циклом RAG-системы, вызывает Mistral API, формирует структурированный ответ.
- **RAG Pipeline**: модуль поиска и генерации:
  - **Семантический чанкинг** — разбиение глав на смысловые блоки с помощью `cointegrated/rubert-tiny2`.
  - **Гибридный ретривер** — объединяет результаты векторного поиска (Chroma) и ключевого поиска (BM25).
  - **Расширение запроса** — LLM генерирует синонимы для улучшения релевантности поиска.
- **Mistral AI Cloud API**: внешний LLM для генерации ответов по контексту. Используется модель `mistral-small-latest`.
- **Локальное векторное хранилище**: Chroma DB с предварительно проиндексированными чанками и метаданными (`book`, `chapter`).

### 4.2. Описание инфраструктуры и масштабируемости

#### Инфраструктура:
- **Frontend**: запускается локально через Vite (`localhost:5173`) или в Docker-контейнере (`localhost:8080`).
- **Backend**: FastAPI-сервер на Python, запускается на CPU (в т.ч. на обычном ноутбуке).
- **Embedding-модель**: `cointegrated/rubert-tiny2` - легковесная модель для русского языка, работает на CPU.
- **Хранилище**: Chroma DB хранится локально в памяти или на диске (в MVP — in-memory).
- **LLM**: внешний сервис Mistral AI (облако), не требует GPU.

#### Масштабируемость:
- **Горизонтальное масштабирование**: возможно при выносе Chroma в отдельный сервер и использовании Redis-очереди для запросов к Mistral API.
- **Кэширование**: можно добавить кэш (например, Redis) для часто задаваемых вопросов — не реализовано в MVP.
- **Ограничения**: зависимость от Mistral API (ограничения rate-limit’ов). Для продакшена потребуется fallback-логика или очередь с retry.

### 4.3. Требования к работе системы

- **SLA**: время ответа ≤ 15 секунд (включая задержку Mistral API).
- **Пропускная способность**: ~1–2 запроса/сек на одном CPU-инстансе (ограничено скоростью embedding и Mistral API).
- **Надёжность**:
  - Обработка ошибок Mistral API (HTTP 429, 5xx) с повторными попытками (`max_retries=2`).
  - Graceful degradation: при ошибке — возврат JSON с полем `"error"`.
- **Масштабирование**: при увеличении нагрузки — горизонтальное масштабирование backend-инстансов + вынос Chroma в отдельный сервис.

### 4.4. Безопасность системы

- **Уязвимости**:
  - Возможна SSRF при неправильной настройке прокси (в текущей архитектуре — нет).
  - Зависимость от внешнего API (Mistral): риск DDoS или компрометации API-ключа.
- **Меры**:
  - API-ключ хранится в `.env`, не коммитится.
  - CORS ограничен только доверенными origin (`localhost:5173`, `localhost:8080`).

### 4.5. Безопасность данных

- Данные книг используются только в образовательных целях, без распространения.
- Нет персональных данных пользователей — история чата хранится локально в `localStorage`.
- Соответствует GDPR: данные не передаются третьим лицам, кроме Mistral AI (текст запроса и контекста).

### 4.6. Издержки

- **Mistral API**: бесплатный тариф (до 50K токенов/мин), достаточно для MVP и пилота.
- **Вычисления**: всё работает на CPU; нет затрат на GPU.
- **Оценка месячных расходов при нагрузке 100 запросов/день**: ~$0 (в рамках бесплатной версии Mistral).

### 4.7. Integration points

- **Frontend → Backend**: `POST /ask` с телом `{"question": "..."}` → возвращает `{"answer": "..."}`.
- **Backend → Mistral AI**: HTTP-запрос через `langchain_mistralai.ChatMistralAI`.
- **Backend ↔ Chroma**: локальный вызов `similarity_search`.

### 4.8. Риски

| Риск                          | Вероятность | Последствия                         | Митигация                              |
|------------------------------|-------------|-------------------------------------|----------------------------------------|
| Rate limit Mistral API       | Высокая     | Отказ в обслуживании                | Retry + fallback на "Не знаю"          |
| Hallucinations LLM           | Средняя     | Неточная цитата                     | Жёсткий промпт + ручная проверка       |
| Низкая релевантность поиска  | Средняя     | Ответ "Не знаю" на известный факт   | Гибридный поиск + RRF                  |
| Зависимость от одного источника текста | Низкая | Ошибки в метаданных                 | Валидация `data_preparation.py`        |